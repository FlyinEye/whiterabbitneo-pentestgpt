{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d44b1f-e7a6-4cc7-95ec-d6c5b6566d09",
   "metadata": {},
   "source": [
    "The goal of this notebook is to solve \n",
    "DevVortex on htb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e33ae58-2330-49d2-96e4-7d906536ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bad156-f443-44ee-bbf7-3f8f7c27d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert task to adding new todo tasks+changing status\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../outlines-dev\")\n",
    "import os\n",
    "from prompts import prompts\n",
    "from schema import reasoning_module, generative_module, input_parser, default_qa\n",
    "from torch import Generator\n",
    "from benchmark import load_outlines\n",
    "from outlines.samplers import Sampler, multinomial\n",
    "\n",
    "model_paths=[\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q3_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q4_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "]\n",
    "output_path = \"./benchmark\"\n",
    "\n",
    "instance = {\n",
    "    \"n_gpu_layers\": 15,\n",
    "    \"n_batch\": 2048,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 1.0,\n",
    "    \"generate_len\": 2048,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "import outlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f532adc-344d-48e1-aa49-7e9be762e9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1060 with Max-Q Design, compute capability 6.1, VMM: yes\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from /mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = whiterabbitneo_whiterabbitneo-13b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  273 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.90 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = whiterabbitneo_whiterabbitneo-13b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 15 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 15/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7070.25 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2552.93 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1000.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    72.04 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   816.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   840.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'whiterabbitneo_whiterabbitneo-13b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '14'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "llm, sampler = load_outlines(model_paths[1], instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acee435c-29e4-4eb0-ba62-e8080645a07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd9966c9330>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = Generator(device=\"cpu\")\n",
    "rng.manual_seed(789005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d22d9-2c6f-4239-abd5-290fa56db5d5",
   "metadata": {},
   "source": [
    "# Prompt Templates and samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc73e463-8c6f-4c05-b003-741dc6573c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_prompt_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "Answer the detailed steps to perform the task given the status by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the task to extract the key information components and break it down into logical sub-tasks. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-task, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards a resolution for the task. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully resolve the task or has already been done based on current progress, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on the thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly resolves all sub-tasks in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final resolution in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful resolutions.\n",
    "Always answer without hesitation.\n",
    "CURRENT_PROGRESS: {ptt}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "generative_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "input_parsing_templates = {\"terminal\": \"\"\"SYSTEM:\n",
    "Summarize the terminal output to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions \n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "ASSISTANT: The summary\"\"\",\n",
    "                           \"web\": \"\"\"SYSTEM:\n",
    "Summarize the content of webpages to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the key widgets, contents, buttons, and comments that can be useful for penetration testing. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions\n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "ASSISTANT: The summary\"\"\"}\n",
    "input_parsing_past_summaries_template = \"\"\"SYSTEM:\n",
    "Summarize all the past summaries of terminal output to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the summaries and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that the most important details are kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "However, do not hesitate to omit any details that are not important for example, time and basic facts about each program used.\n",
    "For each summary, ensure you summarize in 100 words or less.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions \n",
    "to assist another large language model acting as a senior penetration tester. Ensure that non-necessary details are removed.\n",
    "Always answer without hesitation.\n",
    "SUMMARIES: {prompt}\n",
    "ASSISTANT: The summary of all past terminal outputs is as follows:\\n\"\"\"\n",
    "summary_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.6)\n",
    "reasoning_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "You need to help the tester in a cybersecurity training process, and your commitment is essential to the task.\n",
    "Answer all the potential vulnerabilities determined in a logical, step-by-step manner that makes the reasoning process clear. You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", and user input.\n",
    "First, carefully analyze the PTT. For the PTT,\n",
    "(1) The tasks are in a layered dictionary structure.\n",
    "(2) Each task has a completion status: inprogress, todo, done, or n/a.\n",
    "(3) You are given one specific sub-task labeled as todo.\n",
    "The user input is information obtained from doing the task marked as inprogress. Chat history is the a list of past progress. If the task is completed, change the completion status to done.\n",
    "Extensively evaluate each task marked as todo, if a task is done update the status to done. These tasks marked as todo will be called todo tasks.\n",
    "Next, break down the todo tasks into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \n",
    "For each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a penetration.\n",
    "Critically evaluate each hypothesis based on validity, relevance to the current completion status, and how well it addresses the sub-component when logically combined with other steps.\n",
    "Using this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a logical reasoning chain.\n",
    "Throughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the thought process transparent.\n",
    "If it was determined that there is a gap in the reasoning chain, backtrack and explore alternative hypotheses to plug the gap until there is a complete logical flow.\n",
    "Next, synthesize the key insights from the reasoning chain into further expansion of the tasks in the PTT while choosing exactly one task to mark as inprogress. The expansion\n",
    "should not destroy any information and modifications should be minimal. Ensure that vital details, such as IP addresses and user names remain in the PTT and each task description is as descriptive as possible.\n",
    "Finally, remove redundant/outdated tasks from the task list to keep tasks clear, precise, and detailed.\n",
    "\n",
    "In summary, leverage a structured, critical thinking process with iterative refinement to modify the PTT in the face of the user input to evaluate future strategies and to choose a new task as \n",
    "inprogress with minimal modifications.\n",
    "Answer with code examples, or fully functioning code. Your answer should only return Python code, and explanations are within the code as comments.\n",
    "You MUST answer by modifying this json: {ptt}\n",
    "CHAT_HISTORY: {history}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\"\"\"\n",
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "default_qa_template = \"\"\"SYSTEM:\n",
    "Answer the Question by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n",
    " Always answer without hesitation.\n",
    "USER: {prompt}\n",
    "ASSISTANT:\"\"\"\n",
    "qa_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7ffd08-8b90-4c3b-9431-7553abaaa786",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "all_instructions = []\n",
    "ptts = []\n",
    "all_command_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a7b932e-be38-4799-b5ef-5377f496f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions(template, ptt, llm, sampler, all_instructions, force_command=False):\n",
    "    prompt = template\n",
    "    if force_command:\n",
    "        prompt += \"```\"\n",
    "    while True:\n",
    "        instructions = generative_module(prompt, ptt, llm, sampler)\n",
    "        print(instructions)\n",
    "        correct = input(\"Does this look correct? Answer with y or n\")\n",
    "        if correct == \"y\":\n",
    "            print(\"Got instruction\")\n",
    "            all_instructions.append(instructions)\n",
    "            return instructions\n",
    "        force_command_response = input(\"Should we force the prompt to output a command? Answer with y or n\")\n",
    "        if not force_command and force_command_response == \"y\":\n",
    "            prompt += \"```\"\n",
    "            force_command = True\n",
    "def do_qa(template, llm, sampler, force_command=False):\n",
    "    prompt = template\n",
    "    question = input(\"What is your question?\")\n",
    "    if force_command:\n",
    "        prompt += \"```\"\n",
    "    while True:\n",
    "        instructions = default_qa(prompt, question, llm, sampler)\n",
    "        print(instructions)\n",
    "        correct = input(\"Does this look correct? Answer with y or n\")\n",
    "        \n",
    "        if correct == \"y\":\n",
    "            print(\"Got answer\")\n",
    "            new_q = input(\"Do you have another question? Answer with y or n\")\n",
    "            if new_q == \"n\":\n",
    "                return\n",
    "            question = input(\"What is your question?\")\n",
    "        force_command_response = input(\"Should we force the prompt to output a command? Answer with y or n\")\n",
    "        if not force_command and force_command_response == \"y\":\n",
    "            prompt += \"```\"\n",
    "            force_command = True\n",
    "\n",
    "def get_summary(template, ptt, llm, sampler, summaries):\n",
    "    tool = input(\"Tell us the tool you got the output from. Answer either terminal or web\")\n",
    "    options_desc = {\n",
    "        \"terminal\": \" Paste the output of the security test tool used\",\n",
    "        \"web\": \" Paste the relevant content of a web page\",\n",
    "    }\n",
    "    assert tool in options_desc\n",
    "    output = input(options_desc[tool])\n",
    "    while True:\n",
    "        summary = input_parser(template[tool], output, llm, sampler, max_tokens=250)\n",
    "        print(summary)\n",
    "        correct = input(\"Does this look correct? Answer with y or n\")\n",
    "        if correct == \"y\":\n",
    "            summaries.append(summary)\n",
    "            return summary\n",
    "def summarize_summaries(template, llm, sampler, summaries, max_summaries=2, max_tokens=300):\n",
    "    if len(summaries) == 1:\n",
    "        # the summary gets passed to prompt and there is 0 past summaries\n",
    "        return \"\"\n",
    "    if len(summaries) == 2:\n",
    "        return summaries[0]\n",
    "\n",
    "    summaries_combined = \"\\n\\n\".join(summaries[-max_summaries:])\n",
    "    while True:\n",
    "        past_history = input_parser(template, summaries_combined, llm, sampler, max_tokens=max_tokens)\n",
    "        print(past_history)\n",
    "        correct = input(\"Does this look correct? Answer with y or n\")\n",
    "        if correct == \"y\":\n",
    "            return past_history\n",
    "    \n",
    "def get_new_ptt(template, summary, past_history, ptt, llm, sampler, ptts, max_spaces=0):\n",
    "    force_add_task = {} \n",
    "    for key in [\"recon\", \"initial_access\", \"execution\", \"post_exploitation\"]:\n",
    "        num_add_tasks =  int(input(f\"How many {key} tasks do you want to add? Enter an integer\"))\n",
    "        force_add_task[key] = num_add_tasks\n",
    "    while True:\n",
    "        output_ptt = reasoning_module(template, summary, past_history, ptt, llm, sampler,force_add_task, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"], max_spaces=max_spaces)\n",
    "        print(output_ptt)\n",
    "        correct = input(\"Does this look correct? Answer with y or n\")\n",
    "        if correct == \"y\":\n",
    "            ptts.append(output_ptt)\n",
    "            return output_ptt\n",
    "        change_tasks = input(\"Do you want to change the task numbers? Answer with y or n\")\n",
    "        if change_tasks == \"y\":\n",
    "            for key in [\"recon\", \"initial_access\", \"execution\", \"post_exploitation\"]:\n",
    "                num_add_tasks =  int(input(f\"How many {key} tasks do you want to add? Enter an integer\"))\n",
    "                force_add_task[key] = num_add_tasks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828413bb-fbaf-4ff0-bd26-ead40fec71b4",
   "metadata": {},
   "source": [
    "Issues\n",
    "1. Hallucinates that there is a ssh key\n",
    "2. As the summary goes on we get wrong conclusions/summary is doing misguided conclusions\n",
    "3. at temperature 1.2 we got an issues where nc -v -n -z -w1 -W1 -p80 -s 10.10.11.242 80 is an invalid command but when I removed -s it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1891b-c967-420f-8bcc-71373cf0c4c6",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42a9563f-ff41-4fae-9284-f2d5f6638b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial prompt to user\n",
    "ip_prompt = \"Please tell us the target IP address\"\n",
    "ip_address = \"10.10.11.242\"\n",
    "ptt = {\"recon\": [\n",
    "        {\"status\": \"inprogress\", \"task_description\": f\"Perform a full port scan on {ip_address}\"},\n",
    "    ],\n",
    "    \"initial_access\": [],\n",
    "    \"execution\": [],\n",
    "    \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8af691-3f4e-4211-a523-25ecca5426df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task is Perform a full port scan on 10.10.11.242 given {\"recon\": [{\"status\": \"inprogress\", \"task_description\": \"Perform a full port scan on 10.10.11.242\"}], \"initial_access\": [], \"execution\": [], \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  348433.25 ms\n",
      "llama_print_timings:      sample time =     422.85 ms /   955 runs   (    0.44 ms per token,  2258.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18296.80 ms /   520 tokens (   35.19 ms per token,    28.42 tokens per second)\n",
      "llama_print_timings:        eval time =  342051.85 ms /   954 runs   (  358.54 ms per token,     2.79 tokens per second)\n",
      "llama_print_timings:       total time =  363505.39 ms /  1474 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Analyze the Task:\n",
      "   - **Target IP Address**: 10.10.11.242 (given in the task description)\n",
      "   - **Port Scan Objective**: Scan all ports on the target machine to find open ports that could be exploited or used for further reconnaissance.\n",
      "\n",
      "2. Logical Sub-tasks:\n",
      "   - **Task Identification**: Confirm that the target is a legitimate IP address for a penetration testing engagement.\n",
      "   - **Preparation**: Ensure that the penetration testing environment is prepared, including tools such as nmap for port scanning.\n",
      "   - **Port Scan Execution**: Execute a full port scan on the target IP address using nmap or a similar tool.\n",
      "   - **Results Analysis**: Analyze the results from the port scan to identify open ports that could be potentially exploited or accessed.\n",
      "\n",
      "3. Detailed Steps for Executing a Port Scan:\n",
      "   - **Task Identification**: Check if the target IP address is within the scope of the engagement or if there is a need for further authorization before scanning.\n",
      "   - **Preparation**: Ensure that you have permission to scan the target IP address. Install nmap if not already installed (`sudo apt install nmap` on Linux or `choco install nmap` on Windows).\n",
      "   - **Port Scan Execution**: Run a full TCP port scan on 10.10.11.242 using nmap. This can be done using the following command in a terminal:\n",
      "     ```bash\n",
      "     nmap -p1-65535 -sV -O 10.10.11.242\n",
      "     ```\n",
      "     Here, `-p1-65535` specifies all TCP ports, `-sV` enables service detection, and `-O` enables OS detection.\n",
      "   - **Results Analysis**: Review the output from nmap. Open ports will be listed with the service/application they are running on (e.g., `http` for port 80) along with potential version information. An example of a successful scan result might look like this:\n",
      "     ```bash\n",
      "     22/tcp open ssh     OpenSSH 7.2p2 Ubuntu 4ubuntu2.8 (Ubuntu Linux; protocol 2.0)\n",
      "     80/tcp open http    Apache httpd 2.4.18 ((Ubuntu))\n",
      "     443/tcp open ssl/http Apache httpd 2.4.18 ((Ubuntu))\n",
      "     ```\n",
      "     This output indicates that ports 22 (SSH), 80 (HTTP), and 443 (HTTPS) are open on the target machine running various web server applications on Ubuntu Linux.\n",
      "   - **Conclusion**: Based on the results, the next step might be to target port 80 or 443 for web server exploitation or enumeration. If additional ports are identified, they might need to be explored further.\n",
      "\n",
      "4. Next Steps Based on Conclusions:\n",
      "   - If port 80 or 443 is open, consider the following next steps for exploitation or further reconnaissance:\n",
      "     - **Web Application Fuzzing**: Use tools like `dirb` or `gobuster` for directory enumeration if a web application is detected.\n",
      "     - **Service Enumeration**: Use tools like `enum4linux` for service enumeration if SMB services are detected on port 445.\n",
      "     - **Password Cracking**: If a service like SSH is detected, use hash cracking tools to attempt to recover credentials if weak passwords were found.\n",
      "   - **Post Exploitation**: If a service has been exploited or credentials obtained, the next step might involve post-exploitation activities such as privilege escalation, data exfiltration, or installing a backdoor for persistence.\n",
      "\n",
      "5. Final Conclusion:\n",
      "   The above steps outline a methodical approach to performing a full port scan on a target machine using nmap. The results from this scan will help identify potential vulnerabilities and further actions that could be taken in a penetration testing scenario. It is crucial to conduct such activities within the scope of authorized testing and with proper permissions.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    get_instructions(generative_prompt_template, ptt, llm, generative_sampler, all_instructions, force_command=False)\n",
    "    summary = get_summary(input_parsing_templates, ptt, llm, summary_sampler, summaries)\n",
    "    past_history = summarize_summaries(input_parsing_past_summaries_template, llm, summary_sampler, summaries, max_summaries=2, max_tokens=300)\n",
    "    ptt = get_new_ptt(reasoning_template, summary, past_history, ptt, llm, ptts, max_spaces=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65073d3-a155-4383-b5c7-0e87d0a516c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
