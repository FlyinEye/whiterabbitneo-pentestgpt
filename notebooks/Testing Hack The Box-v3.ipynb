{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d44b1f-e7a6-4cc7-95ec-d6c5b6566d09",
   "metadata": {},
   "source": [
    "The goal of this notebook is to solve \n",
    "DevVortex on htb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e33ae58-2330-49d2-96e4-7d906536ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6bad156-f443-44ee-bbf7-3f8f7c27d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert task to adding new todo tasks+changing status\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../outlines-dev\")\n",
    "import os\n",
    "from schema import reasoning_module, generative_module, input_parser, default_qa, load_outlines\n",
    "from torch import Generator\n",
    "from outlines.samplers import Sampler, multinomial\n",
    "import pickle\n",
    "\n",
    "model_paths=[\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q3_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q4_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "]\n",
    "output_path = \"./benchmark\"\n",
    "\n",
    "instance = {\n",
    "    \"n_gpu_layers\": 15,\n",
    "    \"n_batch\": 2048,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 1.0,\n",
    "    \"generate_len\": 2048,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "import outlines\n",
    "import pickle\n",
    "progress_save_path = 'progress.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f532adc-344d-48e1-aa49-7e9be762e9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1060 with Max-Q Design, compute capability 6.1, VMM: yes\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from /mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = whiterabbitneo_whiterabbitneo-13b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  273 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.90 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = whiterabbitneo_whiterabbitneo-13b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 15 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 15/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7070.25 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2552.93 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1000.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    72.04 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   816.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   840.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'whiterabbitneo_whiterabbitneo-13b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '14'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "llm, sampler = load_outlines(model_paths[1], instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acee435c-29e4-4eb0-ba62-e8080645a07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7b88663090>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = Generator(device=\"cpu\")\n",
    "rng.manual_seed(789005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d22d9-2c6f-4239-abd5-290fa56db5d5",
   "metadata": {},
   "source": [
    "# Prompt Templates and samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc73e463-8c6f-4c05-b003-741dc6573c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_prompt_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant.\n",
    "Answer the detailed steps to perform the task from the user by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the task to extract the key information components and break it down into logical sub-tasks. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-task, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards a resolution for the task. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully resolve the task or has already been done based on the chat history, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on the thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly resolves all sub-tasks in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final resolution in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful resolutions.\n",
    "Always answer without hesitation.\n",
    "CURRENT_PROGRESS: {ptt}\n",
    "CHAT_HISTORY: {history}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "generative_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "input_parsing_templates = {\"terminal\": \"\"\"SYSTEM:\n",
    "Summarize the terminal output to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions \n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "ASSISTANT: The summary\"\"\",\n",
    "                           \"web\": \"\"\"SYSTEM:\n",
    "Summarize the content of webpages to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the key widgets, contents, buttons, and comments that can be useful for penetration testing. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions\n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "ASSISTANT: The summary\"\"\"}\n",
    "input_parsing_past_summaries_template = \"\"\"SYSTEM:\n",
    "Summarize all the past summaries of terminal output to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the summaries and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that the most important details are kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "However, do not hesitate to omit any details that are not important for example, time and basic facts about each program used.\n",
    "For each summary, ensure you summarize in 100 words or less.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions \n",
    "to assist another large language model acting as a senior penetration tester. Ensure that non-necessary details are removed.\n",
    "Always answer without hesitation.\n",
    "SUMMARIES: {prompt}\n",
    "ASSISTANT: The summary of all past terminal outputs is as follows:\\n\"\"\"\n",
    "summary_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.6)\n",
    "reasoning_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "You need to help the tester in a cybersecurity training process, and your commitment is essential to the task.\n",
    "Answer all the potential vulnerabilities determined in a logical, step-by-step manner that makes the reasoning process clear. You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", user input, and chat history.\n",
    "First, carefully analyze the PTT. For the PTT,\n",
    "(1) The tasks are in a layered dictionary structure.\n",
    "(2) Each task has a completion status: inprogress, todo, done, or n/a.\n",
    "(3) You are given one specific sub-task labeled as todo.\n",
    "The user input is information obtained from doing the task marked as inprogress. Chat history is the past progress. If the task is completed, change the completion status to done.\n",
    "Extensively evaluate each task marked as todo, if a task is done update the status to done. These tasks marked as todo will be called todo tasks.\n",
    "Next, break down the todo tasks into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \n",
    "For each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a penetration.\n",
    "Critically evaluate each hypothesis based on validity, relevance to the current completion status, and how well it addresses the sub-component when logically combined with other steps.\n",
    "Using this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a logical reasoning chain.\n",
    "Throughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the thought process transparent.\n",
    "If it was determined that there is a gap in the reasoning chain, backtrack and explore alternative hypotheses to plug the gap until there is a complete logical flow.\n",
    "Next, synthesize the key insights from the reasoning chain into further expansion of the tasks in the PTT while choosing exactly one task to mark as inprogress. The expansion\n",
    "should not destroy any information and modifications should be minimal. Ensure that vital details, such as IP addresses and user names remain in the PTT and each task description is as descriptive as possible.\n",
    "Finally, remove redundant/outdated tasks from the task list to keep tasks clear, precise, and detailed.\n",
    "\n",
    "In summary, leverage a structured, critical thinking process with iterative refinement to modify the PTT in the face of the user input to evaluate future strategies and to choose a new task as \n",
    "inprogress with minimal modifications.\n",
    "Answer with code examples, or fully functioning code. Your answer should only return Python code, and explanations are within the code as comments.\n",
    "You MUST answer by modifying this json: {ptt}\n",
    "CHAT_HISTORY: {history}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\"\"\"\n",
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "default_qa_template = \"\"\"SYSTEM:\n",
    "Answer the Question by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n",
    " Always answer without hesitation.\n",
    "USER: {prompt}\n",
    "ASSISTANT:\"\"\"\n",
    "qa_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf59818c-aaa0-4e39-9bfa-572920c9bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options(prompt, options):\n",
    "    output = \"\"\n",
    "    assert output not in options\n",
    "    option_str = options[0] + \"/\"\n",
    "    for i, option in enumerate(options):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        option_str += option + \"/\"\n",
    "    option_str = option_str[:-1]\n",
    "    prompt += f\" Answer with {option_str}\"\n",
    "    while output not in options:\n",
    "        output = input(prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a7b932e-be38-4799-b5ef-5377f496f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions(template, ptt, llm, sampler, all_instructions, current_history=\"\", force_command=False):\n",
    "    prompt = template\n",
    "    if force_command:\n",
    "        prompt += \"The commands to do the tasks are ```bash\"\n",
    "    print(prompt)\n",
    "    while True:\n",
    "        instructions = generative_module(prompt, ptt, llm, sampler, current_history)\n",
    "        print(instructions)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            print(\"Got instruction\")\n",
    "            all_instructions.append(instructions)\n",
    "            return instructions\n",
    "        force_command_response = get_options(\"Should we force the output to commands?\", [\"y\", \"n\"])\n",
    "        if not force_command and force_command_response == \"y\":\n",
    "            prompt += \"The commands to do the tasks are ```bash\"\n",
    "            force_command = True\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "def do_qa(template, llm, sampler, force_command=False):\n",
    "    do_question = get_options(\"Do you have questions?\", [\"y\", \"n\"])\n",
    "    if do_question == \"n\":\n",
    "        return\n",
    "    prompt = template\n",
    "    question = input(\"What is your question?\")\n",
    "    if force_command:\n",
    "        prompt += \"The commands to do the tasks are ```bash\"\n",
    "    while True:\n",
    "        instructions = default_qa(prompt, question, llm, sampler)\n",
    "        print(instructions)\n",
    "        correct = get_y_n(\"Does this look correct?\")\n",
    "        \n",
    "        if correct == \"y\":\n",
    "            print(\"Got answer\")\n",
    "            new_q = get_options(\"Do you have another question?\", [\"y\", \"n\"])\n",
    "            if new_q == \"n\":\n",
    "                return\n",
    "            question = input(\"What is your question?\")\n",
    "        force_command_response = get_options(\"Should we force the output to commands?\", [\"y\", \"n\"])\n",
    "        if not force_command and force_command_response == \"y\":\n",
    "            prompt += \"The commands to do the tasks are ```bash\"\n",
    "            force_command = True\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "\n",
    "def get_summary(template, ptt, llm, sampler, summaries, all_command_outputs):\n",
    "    if len(summaries) == len(all_command_outputs):\n",
    "        tool = get_options(\"Tell us the tool you got the output from.\", [\"terminal\", \"web\"])\n",
    "        options_desc = {\n",
    "            \"terminal\": \" Paste the output of the security test tool used\",\n",
    "            \"web\": \" Paste the relevant content of a web page\",\n",
    "        }\n",
    "        assert tool in options_desc\n",
    "        output = input(options_desc[tool])\n",
    "    else:\n",
    "        output = all_command_outputs[-1]\n",
    "    while True:\n",
    "        summary = input_parser(template[tool], output, llm, sampler, max_tokens=250)\n",
    "        print(summary)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            summaries.append(summary)\n",
    "            all_command_outputs.append(output)\n",
    "            return summary\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "def summarize_summaries(template, llm, sampler, summaries, max_summaries=2, max_tokens=300, full=False):\n",
    "    if len(summaries) == 1:\n",
    "        if full:\n",
    "            return summaries[0]\n",
    "        return \"\"\n",
    "    if len(summaries) == 2:\n",
    "        if not full:\n",
    "            return summaries[0]\n",
    "    if full:\n",
    "        offset= 0\n",
    "    else:\n",
    "        offset= 1\n",
    "    summaries_combined = \"\\n\\n\".join(summaries[-max_summaries-offset:-offset])\n",
    "    while True:\n",
    "        past_history = input_parser(template, summaries_combined, llm, sampler, max_tokens=max_tokens)\n",
    "        print(past_history)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            return past_history\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "    \n",
    "def get_new_ptt(template, summary, past_history, ptt, llm, sampler, ptts, max_spaces=0):\n",
    "    force_add_task = {} \n",
    "    for key in [\"recon\", \"initial_access\", \"execution\", \"post_exploitation\"]:\n",
    "        num_add_tasks =  int(input(f\"How many {key} tasks do you want to add? Enter an integer\"))\n",
    "        force_add_task[key] = num_add_tasks\n",
    "    while True:\n",
    "        output_ptt = reasoning_module(template, summary, past_history, ptt, llm, sampler,force_add_task=force_add_task, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"], max_spaces=max_spaces)\n",
    "        print(output_ptt)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            ptts.append(output_ptt)\n",
    "            return output_ptt\n",
    "        change_tasks = get_options(\"Do you want to change the task numbers?\", [\"y\", \"n\"])\n",
    "        if change_tasks == \"y\":\n",
    "            for key in [\"recon\", \"initial_access\", \"execution\", \"post_exploitation\"]:\n",
    "                num_add_tasks =  int(input(f\"How many {key} tasks do you want to add? Enter an integer\"))\n",
    "                force_add_task[key] = num_add_tasks\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828413bb-fbaf-4ff0-bd26-ead40fec71b4",
   "metadata": {},
   "source": [
    "Issues\n",
    "1. Hallucinates that there is a ssh key\n",
    "2. As the summary goes on we get wrong conclusions/summary is doing misguided conclusions\n",
    "3. at temperature 1.2 we got an issues where nc -v -n -z -w1 -W1 -p80 -s 10.10.11.242 80 is an invalid command but when I removed -s it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1891b-c967-420f-8bcc-71373cf0c4c6",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a9563f-ff41-4fae-9284-f2d5f6638b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial prompt to user\n",
    "ip_prompt = \"Please tell us the target IP address\"\n",
    "ip_address = \"10.10.11.242\"\n",
    "ptt = {\"recon\": [\n",
    "        {\"status\": \"inprogress\", \"task_description\": f\"Perform a full port scan on {ip_address}\"},\n",
    "    ],\n",
    "    \"initial_access\": [],\n",
    "    \"execution\": [],\n",
    "    \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]\n",
    "}\n",
    "reset = False\n",
    "force_command = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7ffd08-8b90-4c3b-9431-7553abaaa786",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(progress_save_path) and not reset:\n",
    "    with open(progress_save_path, 'rb') as handle:\n",
    "        progress = pickle.load(handle)\n",
    "    summaries = progress[\"summaries\"]\n",
    "    all_instructions = progress[\"all_instructions\"]\n",
    "    ptts = progress[\"ptts\"]\n",
    "    all_command_outputs = progress[\"all_command_outputs\"]\n",
    "    current_history = \"\"\n",
    "    if \"current_history\" in progress:\n",
    "        current_history = progress[\"current_history\"]\n",
    "    if len(summaries) > 0:\n",
    "        summary = summaries[-1]\n",
    "    if len(ptts) > 0:\n",
    "        ptt = ptts[-1]\n",
    "else:\n",
    "    summaries = []\n",
    "    all_instructions = []\n",
    "    ptts = []\n",
    "    all_command_outputs = []\n",
    "    current_history = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e020ecf-c585-4505-aba4-03c5cf10cc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries), len(all_instructions), len(ptts), len(all_command_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8af691-3f4e-4211-a523-25ecca5426df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many recon tasks do you want to add? Enter an integer 1\n",
      "How many initial_access tasks do you want to add? Enter an integer 1\n",
      "How many execution tasks do you want to add? Enter an integer 0\n",
      "How many post_exploitation tasks do you want to add? Enter an integer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  324537.61 ms\n",
      "llama_print_timings:      sample time =       1.86 ms /     2 runs   (    0.93 ms per token,  1075.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  324536.69 ms /  1010 tokens (  321.32 ms per token,     3.11 tokens per second)\n",
      "llama_print_timings:        eval time =  212847.26 ms /     1 runs   (212847.26 ms per token,     0.00 tokens per second)\n",
      "llama_print_timings:       total time =  537467.35 ms /  1011 tokens\n",
      "\n",
      "llama_print_timings:        load time =  324537.61 ms\n",
      "llama_print_timings:      sample time =      26.82 ms /    68 runs   (    0.39 ms per token,  2535.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =  191063.75 ms /  1011 tokens (  188.98 ms per token,     5.29 tokens per second)\n",
      "llama_print_timings:        eval time =  211494.08 ms /    67 runs   ( 3156.63 ms per token,     0.32 tokens per second)\n",
      "llama_print_timings:       total time =  403354.25 ms /  1078 tokens\n",
      "\n",
      "llama_print_timings:        load time =  324537.61 ms\n",
      "llama_print_timings:      sample time =       0.76 ms /     2 runs   (    0.38 ms per token,  2635.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33997.16 ms /  1041 tokens (   32.66 ms per token,    30.62 tokens per second)\n",
      "llama_print_timings:        eval time =     343.79 ms /     1 runs   (  343.79 ms per token,     2.91 tokens per second)\n",
      "llama_print_timings:       total time =   34350.57 ms /  1042 tokens\n",
      "\n",
      "llama_print_timings:        load time =  324537.61 ms\n",
      "llama_print_timings:      sample time =      28.71 ms /    71 runs   (    0.40 ms per token,  2473.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  105906.43 ms /  1048 tokens (  101.06 ms per token,     9.90 tokens per second)\n",
      "llama_print_timings:        eval time =  232439.78 ms /    70 runs   ( 3320.57 ms per token,     0.30 tokens per second)\n",
      "llama_print_timings:       total time =  339196.68 ms /  1118 tokens\n",
      "\n",
      "llama_print_timings:        load time =  324537.61 ms\n",
      "llama_print_timings:      sample time =       0.75 ms /     2 runs   (    0.38 ms per token,  2659.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29619.94 ms /  1081 tokens (   27.40 ms per token,    36.50 tokens per second)\n",
      "llama_print_timings:        eval time =     346.31 ms /     1 runs   (  346.31 ms per token,     2.89 tokens per second)\n",
      "llama_print_timings:       total time =   29971.61 ms /  1082 tokens\n",
      "\n",
      "llama_print_timings:        load time =  324537.61 ms\n",
      "llama_print_timings:      sample time =       0.76 ms /     2 runs   (    0.38 ms per token,  2638.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27256.08 ms /  1117 tokens (   24.40 ms per token,    40.98 tokens per second)\n",
      "llama_print_timings:        eval time =     347.53 ms /     1 runs   (  347.53 ms per token,     2.88 tokens per second)\n",
      "llama_print_timings:       total time =   27619.99 ms /  1118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}, {'status': 'inprogress', 'task_description': 'Further investigate SSH service version OpenSSH 8.2p1'}], 'initial_access': [{'status': 'todo', 'task_description': 'Exploit known vulnerabilities in SSH service version OpenSSH 8.2p1'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n",
      "Number of inprogress tasks:  1\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}, {'status': 'inprogress', 'task_description': 'Further investigate SSH service version OpenSSH 8.2p1'}], 'initial_access': [{'status': 'todo', 'task_description': 'Exploit known vulnerabilities in SSH service version OpenSSH 8.2p1'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Does this look correct? Answer with y/n y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM:\n",
      "You're an excellent cybersecurity penetration tester assistant.\n",
      "Answer the detailed steps to perform the task from the user by exploring multiple reasoning paths as follows:\n",
      "- First, carefully analyze the task to extract the key information components and break it down into logical sub-tasks. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
      "- For each sub-task, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards a resolution for the task. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
      "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
      "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
      "- If the current chain is determined to not fully resolve the task or has already been done based on the chat history, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
      "- Throughout the reasoning process, aim to provide explanatory details on the thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
      "- Once a reasoning chain is constructed that thoroughly resolves all sub-tasks in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
      "- Please note that while the focus is on the final resolution in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
      "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful resolutions.\n",
      "Always answer without hesitation.\n",
      "CURRENT_PROGRESS: {ptt}\n",
      "CHAT_HISTORY: {history}\n",
      "USER: {prompt}\n",
      "ASSISTANT:\n",
      "The commands to do the tasks are ```bash\n",
      "task is Further investigate SSH service version OpenSSH 8.2p1 given {\"recon\": [{\"status\": \"done\", \"task_description\": \"Perform a full port scan on 10.10.11.242\"}, {\"status\": \"inprogress\", \"task_description\": \"Further investigate SSH service version OpenSSH 8.2p1\"}], \"initial_access\": [{\"status\": \"todo\", \"task_description\": \"Exploit known vulnerabilities in SSH service version OpenSSH 8.2p1\"}], \"execution\": [], \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  324537.61 ms\n",
      "llama_print_timings:      sample time =     490.64 ms /  1163 runs   (    0.42 ms per token,  2370.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23905.44 ms /   846 tokens (   28.26 ms per token,    35.39 tokens per second)\n",
      "llama_print_timings:        eval time =  429631.97 ms /  1162 runs   (  369.73 ms per token,     2.70 tokens per second)\n",
      "llama_print_timings:       total time =  458232.62 ms /  2008 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nmap -sV -p 22 10.10.11.242``` followed by ```bash\n",
      "nmap -sV -p 80 10.10.11.242```. This will provide us with more information about the services running on those ports.\n",
      "\n",
      "Let's take a step-by-step approach to this:\n",
      "\n",
      "1. **Nmap Port Scanning for SSH Service Version**\n",
      "    ```bash\n",
      "    nmap -sV -p 22 10.10.11.242\n",
      "    ```\n",
      "    - `-sV`: Probe open ports to determine service/version info.\n",
      "    - `-p 22`: Scan only port 22 (SSH service).\n",
      "    - `10.10.11.242`: The target IP address.\n",
      "    This command will provide us with the SSH service version (OpenSSH 8.2p1) which is a critical piece of information that can be exploited if a known vulnerability exists for that version.\n",
      "\n",
      "2. **Nmap Port Scanning for HTTP Service Version**\n",
      "    ```bash\n",
      "    nmap -sV -p 80 10.10.11.242\n",
      "    ```\n",
      "    - `-sV`: Probe open ports to determine service/version info.\n",
      "    - `-p 80`: Scan only port 80 (HTTP service).\n",
      "    This command will provide us with more details about the web server running on port 80, including the exact version of nginx (1.18.0), which can help in further exploitation or analysis of the web application.\n",
      "\n",
      "3. **Check for Known Vulnerabilities in OpenSSH 8.2p1**\n",
      "   - Use a vulnerability database like `CVE Details` or `Exploit Database` to check for known vulnerabilities related to OpenSSH 8.2p1. This can be done manually or using automated tools like `searchsploit`.\n",
      "   - If a vulnerability is found that can be exploited, follow up with a more detailed reconnaissance phase to understand how the vulnerability can be exploited programmatically or manually.\n",
      "   - This step should be done with caution and only after confirming that you are in a legal and ethical testing environment with proper authorization.\n",
      "\n",
      "4. **Explore Further Exploitation of SSH Service Version OpenSSH 8.2p1**\n",
      "   - If a vulnerability is found, consider using a tool like `exploit/unix/ssh/openssh_moduli_exploit` from Metasploit, which targets a specific SSH service version issue. This exploit is only for educational purposes and should not be used in a real attack without proper authorization.\n",
      "   - Manual exploitation can also be attempted if a vulnerability is found that can be leveraged with a script or custom exploit code, but again, this should only be done in a controlled environment with proper permissions.\n",
      "   - After exploiting a system, follow up with post-exploitation tasks, such as gathering hashes for password cracking or dumping credentials.\n",
      "\n",
      "5. **Gather Information for Further Post-Exploitation**\n",
      "   - After gaining access to the target system, gather as much information as possible such as usernames, passwords, sensitive files, and configurations. This can be done using tools like `ssh` (for passwords), `cat`, `grep`, `find`, `ls`, `whoami`, `id`, etc., or using specialized post-exploitation tools like `Mimikatz` for Windows systems.\n",
      "   - The goal is to collect as much data as possible to understand the target environment and potential pivot points for further exploitation.\n",
      "   - This information will be used in the post-exploitation phase to escalate privileges, establish persistence, or move laterally within the network.\n",
      "\n",
      "6. **Document All Findings**\n",
      "   - Take notes or screenshots of all actions taken during the reconnaissance process, including versions of services and potential vulnerabilities. This document will be used for reporting findings and will be crucial in providing evidence of the testing done.\n",
      "   - Ensure that all actions taken are documented for accountability and to support any future investigations or reports.\n",
      "\n",
      "7. **Follow Ethical Guidelines**\n",
      "   - Always remember that authorized penetration testing should be conducted with permission and in accordance with all applicable laws and ethical guidelines. Unauthorized testing can lead to legal consequences and ethical issues.\n",
      "\n",
      "8. **Provide a Final Report**\n",
      "   - Once all actions have been taken and findings documented, compile all information into a final report for stakeholders or management. The report should include the methods used, findings, recommendations for improving security, and evidence of the testing process.\n",
      "   - This report will serve as a record of the penetration testing activities for future reference and can be used in training or for legal proceedings.\n",
      "\n",
      "This step-by-step approach provides a structured way to approach security testing tasks, from initial reconnaissance through post-exploitation activities and reporting findings. It is imperative that these actions are taken with caution, ethically, and legally.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        if len(all_instructions) == len(summaries) and len(all_instructions) == len(ptts):\n",
    "            get_instructions(generative_prompt_template, ptt, llm, generative_sampler, all_instructions, current_history=current_history, force_command=force_command)\n",
    "            do_qa(default_qa_template, llm, qa_sampler, force_command=False)\n",
    "        if len(summaries) == len(ptts):\n",
    "            summary = get_summary(input_parsing_templates, ptt, llm, summary_sampler, summaries, all_command_outputs)\n",
    "        if len(current_history) > 0:\n",
    "            past_history = summarize_summaries(input_parsing_past_summaries_template, llm, summary_sampler, summaries, max_summaries=2, max_tokens=300)\n",
    "        else:\n",
    "            past_history = current_history\n",
    "        ptt = get_new_ptt(reasoning_template, summary, past_history, ptt, llm, reasoning_sampler, ptts, max_spaces=0)\n",
    "        current_history = summarize_summaries(input_parsing_past_summaries_template, llm, summary_sampler, summaries, max_summaries=2, max_tokens=300, full=True)\n",
    "        progress = {\n",
    "            \"summaries\": summaries,\n",
    "            \"all_instructions\": all_instructions,\n",
    "            \"ptts\": ptts,\n",
    "            \"all_command_outputs\": all_command_outputs\n",
    "        }\n",
    "        with open(progress_save_path, 'wb') as handle:\n",
    "            pickle.dump(progress, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)\n",
    "    progress = {\n",
    "        \"summaries\": summaries,\n",
    "        \"all_instructions\": all_instructions,\n",
    "        \"ptts\": ptts,\n",
    "        \"all_command_outputs\": all_command_outputs\n",
    "    }\n",
    "    with open(progress_save_path, 'wb') as handle:\n",
    "        pickle.dump(progress, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efda45d-654e-4cbb-9a50-a84a01362f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = {\n",
    "    \"summaries\": summaries,\n",
    "    \"all_instructions\": all_instructions,\n",
    "    \"ptts\": ptts,\n",
    "    \"all_command_outputs\": all_command_outputs,\n",
    "    \"current_history\": current_history\n",
    "}\n",
    "with open(progress_save_path, 'wb') as handle:\n",
    "    pickle.dump(progress, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36ebc5-35f1-485a-b2ea-4935cbe9f9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be156fed-4121-4af3-98dc-59419dce3925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5f1a0-fed1-440a-961f-3016adae0dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
