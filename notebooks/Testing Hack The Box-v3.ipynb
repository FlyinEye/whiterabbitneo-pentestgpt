{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d44b1f-e7a6-4cc7-95ec-d6c5b6566d09",
   "metadata": {},
   "source": [
    "The goal of this notebook is to solve \n",
    "DevVortex on htb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e33ae58-2330-49d2-96e4-7d906536ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bad156-f443-44ee-bbf7-3f8f7c27d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert task to adding new todo tasks+changing status\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../outlines-dev\")\n",
    "import os\n",
    "from prompts import prompts\n",
    "from schema import reasoning_module, generative_module, input_parser, default_qa\n",
    "from torch import Generator\n",
    "from benchmark import load_outlines\n",
    "from outlines.samplers import Sampler, multinomial\n",
    "import pickle\n",
    "\n",
    "model_paths=[\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q3_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q4_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "]\n",
    "output_path = \"./benchmark\"\n",
    "\n",
    "instance = {\n",
    "    \"n_gpu_layers\": 15,\n",
    "    \"n_batch\": 2048,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 1.0,\n",
    "    \"generate_len\": 2048,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "import outlines\n",
    "import pickle\n",
    "progress_save_path = 'progress.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f532adc-344d-48e1-aa49-7e9be762e9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1060 with Max-Q Design, compute capability 6.1, VMM: yes\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from /mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = whiterabbitneo_whiterabbitneo-13b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  273 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.90 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = whiterabbitneo_whiterabbitneo-13b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 15 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 15/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7070.25 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2552.93 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1000.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    72.04 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   816.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   840.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'whiterabbitneo_whiterabbitneo-13b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '14'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "llm, sampler = load_outlines(model_paths[1], instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acee435c-29e4-4eb0-ba62-e8080645a07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff8e10054b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = Generator(device=\"cpu\")\n",
    "rng.manual_seed(789005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d22d9-2c6f-4239-abd5-290fa56db5d5",
   "metadata": {},
   "source": [
    "# Prompt Templates and samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc73e463-8c6f-4c05-b003-741dc6573c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_prompt_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant.\n",
    "Answer the detailed steps to perform the task from the user by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the task to extract the key information components and break it down into logical sub-tasks. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-task, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards a resolution for the task. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully resolve the task or has already been done based on the chat history, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on the thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly resolves all sub-tasks in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final resolution in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful resolutions.\n",
    "Always answer without hesitation.\n",
    "CURRENT_PROGRESS: {ptt}\n",
    "CHAT_HISTORY: {history}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "generative_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "input_parsing_templates = {\"terminal\": \"\"\"SYSTEM:\n",
    "Summarize the terminal output to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions \n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "ASSISTANT: The summary\"\"\",\n",
    "                           \"web\": \"\"\"SYSTEM:\n",
    "Summarize the content of webpages to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the key widgets, contents, buttons, and comments that can be useful for penetration testing. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions\n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "ASSISTANT: The summary\"\"\"}\n",
    "input_parsing_past_summaries_template = \"\"\"SYSTEM:\n",
    "Summarize all the past summaries of terminal output to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the summaries and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that the most important details are kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "However, do not hesitate to omit any details that are not important for example, time and basic facts about each program used.\n",
    "For each summary, ensure you summarize in 100 words or less.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions \n",
    "to assist another large language model acting as a senior penetration tester. Ensure that non-necessary details are removed.\n",
    "Always answer without hesitation.\n",
    "SUMMARIES: {prompt}\n",
    "ASSISTANT: The summary of all past terminal outputs is as follows:\\n\"\"\"\n",
    "summary_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.6)\n",
    "reasoning_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "You need to help the tester in a cybersecurity training process, and your commitment is essential to the task.\n",
    "Answer all the potential vulnerabilities determined in a logical, step-by-step manner that makes the reasoning process clear. You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", user input, and chat history.\n",
    "First, carefully analyze the PTT. For the PTT,\n",
    "(1) The tasks are in a layered dictionary structure.\n",
    "(2) Each task has a completion status: inprogress, todo, done, or n/a.\n",
    "(3) You are given one specific sub-task labeled as todo.\n",
    "The user input is information obtained from doing the task marked as inprogress. Chat history is the past progress. If the task is completed, change the completion status to done.\n",
    "Extensively evaluate each task marked as todo, if a task is done update the status to done. These tasks marked as todo will be called todo tasks.\n",
    "Next, break down the todo tasks into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \n",
    "For each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a penetration.\n",
    "Critically evaluate each hypothesis based on validity, relevance to the current completion status, and how well it addresses the sub-component when logically combined with other steps.\n",
    "Using this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a logical reasoning chain.\n",
    "Throughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the thought process transparent.\n",
    "If it was determined that there is a gap in the reasoning chain, backtrack and explore alternative hypotheses to plug the gap until there is a complete logical flow.\n",
    "Next, synthesize the key insights from the reasoning chain into further expansion of the tasks in the PTT while choosing exactly one task to mark as inprogress. The expansion\n",
    "should not destroy any information and modifications should be minimal. Ensure that vital details, such as IP addresses and user names remain in the PTT and each task description is as descriptive as possible.\n",
    "Finally, remove redundant/outdated tasks from the task list to keep tasks clear, precise, and detailed.\n",
    "\n",
    "In summary, leverage a structured, critical thinking process with iterative refinement to modify the PTT in the face of the user input to evaluate future strategies and to choose a new task as \n",
    "inprogress with minimal modifications.\n",
    "Answer with code examples, or fully functioning code. Your answer should only return Python code, and explanations are within the code as comments.\n",
    "You MUST answer by modifying this json: {ptt}\n",
    "CHAT_HISTORY: {history}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\"\"\"\n",
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "default_qa_template = \"\"\"SYSTEM:\n",
    "Answer the Question by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n",
    " Always answer without hesitation.\n",
    "USER: {prompt}\n",
    "ASSISTANT:\"\"\"\n",
    "qa_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf59818c-aaa0-4e39-9bfa-572920c9bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options(prompt, options):\n",
    "    output = \"\"\n",
    "    assert output not in options\n",
    "    option_str = options[0] + \"/\"\n",
    "    for i, option in enumerate(options):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        option_str += option + \"/\"\n",
    "    option_str = option_str[:-1]\n",
    "    prompt += f\" Answer with {option_str}\"\n",
    "    while output not in options:\n",
    "        output = input(prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7b932e-be38-4799-b5ef-5377f496f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions(template, ptt, llm, sampler, all_instructions, current_history=\"\", force_command=False):\n",
    "    prompt = template\n",
    "    if force_command:\n",
    "        prompt += \"The commands to do the tasks are ```bash\"\n",
    "    print(prompt)\n",
    "    while True:\n",
    "        instructions = generative_module(prompt, ptt, llm, sampler, current_history)\n",
    "        print(instructions)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            print(\"Got instruction\")\n",
    "            all_instructions.append(instructions)\n",
    "            return instructions\n",
    "        force_command_response = get_options(\"Should we force the output to commands?\", [\"y\", \"n\"])\n",
    "        if not force_command and force_command_response == \"y\":\n",
    "            prompt += \"The commands to do the tasks are ```bash\"\n",
    "            force_command = True\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "def do_qa(template, llm, sampler, force_command=False):\n",
    "    do_question = get_options(\"Do you have questions?\", [\"y\", \"n\"])\n",
    "    if do_question == \"n\":\n",
    "        return\n",
    "    prompt = template\n",
    "    question = input(\"What is your question?\")\n",
    "    if force_command:\n",
    "        prompt += \"The commands to do the tasks are ```bash\"\n",
    "    while True:\n",
    "        instructions = default_qa(prompt, question, llm, sampler)\n",
    "        print(instructions)\n",
    "        correct = get_y_n(\"Does this look correct?\")\n",
    "        \n",
    "        if correct == \"y\":\n",
    "            print(\"Got answer\")\n",
    "            new_q = get_options(\"Do you have another question?\", [\"y\", \"n\"])\n",
    "            if new_q == \"n\":\n",
    "                return\n",
    "            question = input(\"What is your question?\")\n",
    "        force_command_response = get_options(\"Should we force the output to commands?\", [\"y\", \"n\"])\n",
    "        if not force_command and force_command_response == \"y\":\n",
    "            prompt += \"The commands to do the tasks are ```bash\"\n",
    "            force_command = True\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "\n",
    "def get_summary(template, ptt, llm, sampler, summaries, all_command_outputs):\n",
    "    if len(summaries) == len(all_command_outputs):\n",
    "        tool = get_options(\"Tell us the tool you got the output from.\", [\"terminal\", \"web\"])\n",
    "        options_desc = {\n",
    "            \"terminal\": \" Paste the output of the security test tool used\",\n",
    "            \"web\": \" Paste the relevant content of a web page\",\n",
    "        }\n",
    "        assert tool in options_desc\n",
    "        output = input(options_desc[tool])\n",
    "    else:\n",
    "        output = all_command_outputs[-1]\n",
    "    while True:\n",
    "        summary = input_parser(template[tool], output, llm, sampler, max_tokens=250)\n",
    "        print(summary)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            summaries.append(summary)\n",
    "            all_command_outputs.append(output)\n",
    "            return summary\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "def summarize_summaries(template, llm, sampler, summaries, max_summaries=2, max_tokens=300, full=False):\n",
    "    if len(summaries) == 1:\n",
    "        if full:\n",
    "            return summaries[0]\n",
    "        return \"\"\n",
    "    if len(summaries) == 2:\n",
    "        if not full:\n",
    "            return summaries[0]\n",
    "    if full:\n",
    "        offset= 0\n",
    "    else:\n",
    "        offset= 1\n",
    "    summaries_combined = \"\\n\\n\".join(summaries[-max_summaries-offset:-offset])\n",
    "    while True:\n",
    "        past_history = input_parser(template, summaries_combined, llm, sampler, max_tokens=max_tokens)\n",
    "        print(past_history)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            return past_history\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "    \n",
    "def get_new_ptt(template, summary, past_history, ptt, llm, sampler, ptts, max_spaces=0):\n",
    "    force_add_task = {} \n",
    "    for key in [\"recon\", \"initial_access\", \"execution\", \"post_exploitation\"]:\n",
    "        num_add_tasks =  int(input(f\"How many {key} tasks do you want to add? Enter an integer\"))\n",
    "        force_add_task[key] = num_add_tasks\n",
    "    while True:\n",
    "        output_ptt = reasoning_module(template, summary, past_history, ptt, llm, sampler,force_add_task=force_add_task, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"], max_spaces=max_spaces)\n",
    "        print(output_ptt)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            ptts.append(output_ptt)\n",
    "            return output_ptt\n",
    "        change_tasks = get_options(\"Do you want to change the task numbers?\", [\"y\", \"n\"])\n",
    "        if change_tasks == \"y\":\n",
    "            for key in [\"recon\", \"initial_access\", \"execution\", \"post_exploitation\"]:\n",
    "                num_add_tasks =  int(input(f\"How many {key} tasks do you want to add? Enter an integer\"))\n",
    "                force_add_task[key] = num_add_tasks\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828413bb-fbaf-4ff0-bd26-ead40fec71b4",
   "metadata": {},
   "source": [
    "Issues\n",
    "1. Hallucinates that there is a ssh key\n",
    "2. As the summary goes on we get wrong conclusions/summary is doing misguided conclusions\n",
    "3. at temperature 1.2 we got an issues where nc -v -n -z -w1 -W1 -p80 -s 10.10.11.242 80 is an invalid command but when I removed -s it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1891b-c967-420f-8bcc-71373cf0c4c6",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a9563f-ff41-4fae-9284-f2d5f6638b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial prompt to user\n",
    "ip_prompt = \"Please tell us the target IP address\"\n",
    "ip_address = \"10.10.11.242\"\n",
    "ptt = {\"recon\": [\n",
    "        {\"status\": \"inprogress\", \"task_description\": f\"Perform a full port scan on {ip_address}\"},\n",
    "    ],\n",
    "    \"initial_access\": [],\n",
    "    \"execution\": [],\n",
    "    \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]\n",
    "}\n",
    "reset = False\n",
    "force_command = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a7ffd08-8b90-4c3b-9431-7553abaaa786",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(progress_save_path) and not reset:\n",
    "    with open(progress_save_path, 'rb') as handle:\n",
    "        progress = pickle.load(handle)\n",
    "    summaries = progress[\"summaries\"]\n",
    "    all_instructions = progress[\"all_instructions\"]\n",
    "    ptts = progress[\"ptts\"]\n",
    "    all_command_outputs = progress[\"all_command_outputs\"]\n",
    "    current_history = \"\"\n",
    "    if \"current_history\" in progress:\n",
    "        current_history = progress[\"current_history\"]\n",
    "    if len(summaries) > 0:\n",
    "        summary = summaries[-1]\n",
    "    if len(ptts) > 0:\n",
    "        ptt = ptts[-1]\n",
    "else:\n",
    "    summaries = []\n",
    "    all_instructions = []\n",
    "    ptts = []\n",
    "    all_command_outputs = []\n",
    "    current_history = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e020ecf-c585-4505-aba4-03c5cf10cc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries), len(all_instructions), len(ptts), len(all_command_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8af691-3f4e-4211-a523-25ecca5426df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many recon tasks do you want to add? Enter an integer 1\n",
      "How many initial_access tasks do you want to add? Enter an integer 1\n",
      "How many execution tasks do you want to add? Enter an integer 0\n",
      "How many post_exploitation tasks do you want to add? Enter an integer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =       2.11 ms /     2 runs   (    1.05 ms per token,   947.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =  430401.06 ms /  1010 tokens (  426.14 ms per token,     2.35 tokens per second)\n",
      "llama_print_timings:        eval time =   50584.00 ms /     1 runs   (50584.00 ms per token,     0.02 tokens per second)\n",
      "llama_print_timings:       total time =  481290.25 ms /  1011 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =      28.21 ms /    68 runs   (    0.41 ms per token,  2410.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =  109977.54 ms /  1011 tokens (  108.78 ms per token,     9.19 tokens per second)\n",
      "llama_print_timings:        eval time =   82262.16 ms /    67 runs   ( 1227.79 ms per token,     0.81 tokens per second)\n",
      "llama_print_timings:       total time =  196498.09 ms /  1078 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =       0.77 ms /     2 runs   (    0.38 ms per token,  2614.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29734.54 ms /  1041 tokens (   28.56 ms per token,    35.01 tokens per second)\n",
      "llama_print_timings:        eval time =     348.24 ms /     1 runs   (  348.24 ms per token,     2.87 tokens per second)\n",
      "llama_print_timings:       total time =   30266.59 ms /  1042 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =      28.85 ms /    71 runs   (    0.41 ms per token,  2461.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  110905.55 ms /  1048 tokens (  105.83 ms per token,     9.45 tokens per second)\n",
      "llama_print_timings:        eval time =   80459.65 ms /    70 runs   ( 1149.42 ms per token,     0.87 tokens per second)\n",
      "llama_print_timings:       total time =  192432.41 ms /  1118 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =       0.80 ms /     2 runs   (    0.40 ms per token,  2509.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27880.41 ms /  1081 tokens (   25.79 ms per token,    38.77 tokens per second)\n",
      "llama_print_timings:        eval time =     356.38 ms /     1 runs   (  356.38 ms per token,     2.81 tokens per second)\n",
      "llama_print_timings:       total time =   28304.38 ms /  1082 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =       0.78 ms /     2 runs   (    0.39 ms per token,  2564.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27695.49 ms /  1117 tokens (   24.79 ms per token,    40.33 tokens per second)\n",
      "llama_print_timings:        eval time =     353.46 ms /     1 runs   (  353.46 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time =   28074.31 ms /  1118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}, {'status': 'todo', 'task_description': 'Identify the operating system of the target system based on banner grabbing'}], 'initial_access': [{'status': 'inprogress', 'task_description': 'Use the information from the port scan to find any credentials or services that may be accessible without authentication'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n",
      "Number of inprogress tasks:  1\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}, {'status': 'todo', 'task_description': 'Identify the operating system of the target system based on banner grabbing'}], 'initial_access': [{'status': 'inprogress', 'task_description': 'Use the information from the port scan to find any credentials or services that may be accessible without authentication'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Does this look correct? Answer with y/n y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM:\n",
      "You're an excellent cybersecurity penetration tester assistant.\n",
      "Answer the detailed steps to perform the task from the user by exploring multiple reasoning paths as follows:\n",
      "- First, carefully analyze the task to extract the key information components and break it down into logical sub-tasks. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
      "- For each sub-task, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards a resolution for the task. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
      "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
      "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
      "- If the current chain is determined to not fully resolve the task or has already been done based on the chat history, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
      "- Throughout the reasoning process, aim to provide explanatory details on the thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
      "- Once a reasoning chain is constructed that thoroughly resolves all sub-tasks in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
      "- Please note that while the focus is on the final resolution in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
      "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful resolutions.\n",
      "Always answer without hesitation.\n",
      "CURRENT_PROGRESS: {ptt}\n",
      "CHAT_HISTORY: {history}\n",
      "USER: {prompt}\n",
      "ASSISTANT:\n",
      "The commands to do the tasks are ```bash\n",
      "task is Use the information from the port scan to find any credentials or services that may be accessible without authentication given {\"recon\": [{\"status\": \"done\", \"task_description\": \"Perform a full port scan on 10.10.11.242\"}, {\"status\": \"todo\", \"task_description\": \"Identify the operating system of the target system based on banner grabbing\"}], \"initial_access\": [{\"status\": \"inprogress\", \"task_description\": \"Use the information from the port scan to find any credentials or services that may be accessible without authentication\"}], \"execution\": [], \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =     304.09 ms /   694 runs   (    0.44 ms per token,  2282.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22042.18 ms /   851 tokens (   25.90 ms per token,    38.61 tokens per second)\n",
      "llama_print_timings:        eval time =  255313.67 ms /   693 runs   (  368.42 ms per token,     2.71 tokens per second)\n",
      "llama_print_timings:       total time =  279819.04 ms /  1544 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nmap -sV -T4 -A -p 22,80 10.10.11.242``` to perform a full port scan on ports 22 (SSH) and 80 (HTTP). The options used are:\n",
      "- `-sV`: Probe open ports to determine service/version info.\n",
      "- `-T4`: Sets the timing template to \"aggressive,\" faster than -T5 but slower than -T4. This is appropriate for a target that is expected to respond quickly.\n",
      "- `-A`: Enables OS detection, version detection, script scanning, and traceroute.\n",
      "- `-p 22,80`: Specifies ports to scan; in this case, ports 22 (SSH) and 80 (HTTP).\n",
      "\n",
      "After running this command, you will get a list of open ports along with service/version info. Here is a hypothetical output for reference:\n",
      "\n",
      "```bash\n",
      "Nmap scan report for 10.10.11.242\n",
      "Host is up (0.023s latency).\n",
      "Not shown: 998 closed ports\n",
      "PORT   STATE SERVICE VERSION\n",
      "22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4ubuntu0.9 (Ubuntu Linux; protocol 2.0)\n",
      "80/tcp open  http    nginx 1.18.0\n",
      "|_http-server-header: nginx/1.18.0\n",
      "|_http-title: DevVortex Service Info\n",
      "Service Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n",
      "```\n",
      "\n",
      "From this output, you can see that ports 22 and 80 are open on the target system. The service version information is crucial for further analysis and potential exploitation. However, without proper authentication, you cannot access services running on these ports without additional exploits or vulnerabilities that are known or discovered through other means (e.g., public exploits or vulnerabilities).\n",
      "\n",
      "To find credentials or services that may be accessible without authentication, you would need to conduct further reconnaissance, such as:\n",
      "\n",
      "1. **Service Enumeration**: Attempting to enumerate services on open ports, such as anonymous access or services that have default credentials or weak configurations that can be exploited without proper authentication.\n",
      "2. **Search for Public Exploits**: Checking databases like Exploit Database for known exploits that target the identified service versions or operating systems.\n",
      "3. **Brute Forcing**: If services are not properly secured or have weak passwords, attempting brute force attacks on login forms or SSH.\n",
      "4. **Social Engineering**: If there is a web application running on port 80 with a login form, you might consider using social engineering techniques like phishing or credential stuffing if the target organization is lax about security measures.\n",
      "\n",
      "Always remember that unauthorized access or exploitation of computer systems is illegal and unethical. The above activities should only be performed in a legal context, such as a penetration testing engagement with proper authorization or for security research purposes.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Does this look correct? Answer with y/n y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got instruction\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you have questions? Answer with y/n n\n",
      "Tell us the tool you got the output from. Answer with terminal/web terminal\n",
      " Paste the output of the security test tool used nmap -sV -sC -T4 -A -p 22,80 10.10.11.242 Starting Nmap 7.80 ( https://nmap.org ) at 2024-03-13 09:33 JST Nmap scan report for devvortex.htb (10.10.11.242) Host is up (0.022s latency).  PORT   STATE SERVICE VERSION 22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4ubuntu0.9 (Ubuntu Linux; protocol 2.0) 80/tcp open  http    nginx 1.18.0 (Ubuntu) |_http-server-header: nginx/1.18.0 (Ubuntu) |_http-title: DevVortex Service Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =     121.96 ms /   250 runs   (    0.49 ms per token,  2049.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   88315.59 ms /   506 tokens (  174.54 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:        eval time =  102116.89 ms /   249 runs   (  410.11 ms per token,     2.44 tokens per second)\n",
      "llama_print_timings:       total time =  191777.53 ms /   755 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the nmap scan is:\n",
      "\n",
      "10.10.11.242 (devvortex.htb) is up with an active firewall or network address translation (NAT). The scan took roughly 0.022 seconds to complete.\n",
      "\n",
      "The following services were identified on port 22 (SSH) and 80 (HTTP):\n",
      "\n",
      "- SSH (Port 22): OpenSSH 8.2p1 running on Ubuntu Linux. The protocol version is 2.0, indicating that the server supports SSH2 and SSH1, but not SSH1. The banner includes the string \"Ubuntu Linux\" which is likely a virtual machine running Ubuntu. The string \"protocol 2.0\" implies that SSH2 is supported by this server. The SSH server version is \"OpenSSH 8.2p1 Ubuntu 4ubuntu0.9\". The exact version of Ubuntu is unknown due to truncation in the banner string.\n",
      "- HTTP (Port 80): The server is running an nginx web server version 1.18.0 with Ubuntu Linux (CPE:/o:linux:linux_kernel). The HTTP\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Does this look correct? Answer with y/n y\n",
      "How many recon tasks do you want to add? Enter an integer 2\n",
      "How many initial_access tasks do you want to add? Enter an integer 2\n",
      "How many execution tasks do you want to add? Enter an integer 2\n",
      "How many post_exploitation tasks do you want to add? Enter an integer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /     3 runs   (    0.39 ms per token,  2575.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35959.33 ms /  1352 tokens (   26.60 ms per token,    37.60 tokens per second)\n",
      "llama_print_timings:        eval time =     803.98 ms /     2 runs   (  401.99 ms per token,     2.49 tokens per second)\n",
      "llama_print_timings:       total time =   36940.69 ms /  1354 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /     3 runs   (    0.38 ms per token,  2613.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35309.30 ms /  1392 tokens (   25.37 ms per token,    39.42 tokens per second)\n",
      "llama_print_timings:        eval time =     750.26 ms /     2 runs   (  375.13 ms per token,     2.67 tokens per second)\n",
      "llama_print_timings:       total time =   36162.96 ms /  1394 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =      27.67 ms /    68 runs   (    0.41 ms per token,  2457.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =  233172.61 ms /  1353 tokens (  172.34 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:        eval time =   86800.21 ms /    67 runs   ( 1295.53 ms per token,     0.77 tokens per second)\n",
      "llama_print_timings:       total time =  321004.20 ms /  1420 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =      53.98 ms /    64 runs   (    0.84 ms per token,  1185.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =  138014.98 ms /  1383 tokens (   99.79 ms per token,    10.02 tokens per second)\n",
      "llama_print_timings:        eval time = 12406288.56 ms /    63 runs   (196925.22 ms per token,     0.01 tokens per second)\n",
      "llama_print_timings:       total time = 12545686.91 ms /  1446 tokens\n",
      "\n",
      "llama_print_timings:        load time =  430528.77 ms\n",
      "llama_print_timings:      sample time =       0.76 ms /     2 runs   (    0.38 ms per token,  2642.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66404.93 ms /  1409 tokens (   47.13 ms per token,    21.22 tokens per second)\n",
      "llama_print_timings:        eval time =     381.82 ms /     1 runs   (  381.82 ms per token,     2.62 tokens per second)\n",
      "llama_print_timings:       total time =   66807.98 ms /  1410 tokens\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        if len(all_instructions) == len(summaries) and len(all_instructions) == len(ptts):\n",
    "            get_instructions(generative_prompt_template, ptt, llm, generative_sampler, all_instructions, current_history=current_history, force_command=force_command)\n",
    "            do_qa(default_qa_template, llm, qa_sampler, force_command=False)\n",
    "        if len(summaries) == len(ptts):\n",
    "            summary = get_summary(input_parsing_templates, ptt, llm, summary_sampler, summaries, all_command_outputs)\n",
    "        if len(current_history) > 0:\n",
    "            past_history = summarize_summaries(input_parsing_past_summaries_template, llm, summary_sampler, summaries, max_summaries=2, max_tokens=300)\n",
    "        else:\n",
    "            past_history = current_history\n",
    "        ptt = get_new_ptt(reasoning_template, summary, past_history, ptt, llm, reasoning_sampler, ptts, max_spaces=0)\n",
    "        current_history = summarize_summaries(input_parsing_past_summaries_template, llm, summary_sampler, summaries, max_summaries=2, max_tokens=300, full=True)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)\n",
    "    progress = {\n",
    "        \"summaries\": summaries,\n",
    "        \"all_instructions\": all_instructions,\n",
    "        \"ptts\": ptts,\n",
    "        \"all_command_outputs\": all_command_outputs\n",
    "    }\n",
    "    with open(progress_save_path, 'wb') as handle:\n",
    "        pickle.dump(progress, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efda45d-654e-4cbb-9a50-a84a01362f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = {\n",
    "    \"summaries\": summaries,\n",
    "    \"all_instructions\": all_instructions,\n",
    "    \"ptts\": ptts,\n",
    "    \"all_command_outputs\": all_command_outputs,\n",
    "    \"current_history\": current_history\n",
    "}\n",
    "with open(progress_save_path, 'wb') as handle:\n",
    "    pickle.dump(progress, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36ebc5-35f1-485a-b2ea-4935cbe9f9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be156fed-4121-4af3-98dc-59419dce3925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5f1a0-fed1-440a-961f-3016adae0dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
